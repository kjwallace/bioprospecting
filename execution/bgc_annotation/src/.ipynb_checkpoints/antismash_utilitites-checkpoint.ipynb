{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534aa71b-cf09-4084-8a3d-5f15ff3abd58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(coverage_dict)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mHere we define the function get_coverage\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coverage\u001b[39m(input_gbk : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, input_bam : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \\\n\u001b[0;32m---> 63\u001b[0m                  sample_name : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mdf\u001b[49m:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    input_gbk:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m        type: str \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m        A data frame with the following columns  \u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    1 Perse GBK\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Here we define all the subfunctions that we will be using within our function get_coverage\n",
    "'''\n",
    "\n",
    "'''\n",
    "get_features\n",
    "'''\n",
    "def get_features(record, feature_name, qualifier_name):\n",
    "   for feature in record.features:\n",
    "       if feature.type == feature_name:\n",
    "           return feature.qualifiers[qualifier_name][0]\n",
    "\n",
    "'''\n",
    "get_feature_location\n",
    "'''\n",
    "def get_feature_location(record, feature_name):\n",
    "   for feature in record.features:\n",
    "      if feature.type == feature_name:\n",
    "         return [int(feature.location.start), int(feature.location.end)]\n",
    "\n",
    "'''\n",
    "gbk_parser\n",
    "'''\n",
    "def gbk_parser(input_gbk):\n",
    "    acc2x = dict()  \n",
    "    with open(input_gbk, \"r\") as gbk_handle:\n",
    "        for record in SeqIO.parse(gbk_handle, \"genbank\"):\n",
    "            acc = record.annotations['accessions'][0]\n",
    "            bgc_class = get_features(record,\"cand_cluster\", \"product\")\n",
    "            contg_edge = get_features(record,\"cand_cluster\", \"contig_edge\")  \n",
    "            location = get_feature_location(record,\"cand_cluster\")\n",
    "            acc2x[acc] = {\"acc\": acc, \"bgc_class\": bgc_class, \"contg_edge\": contg_edge, \"start\":location[0], \"end\":location[1]}\n",
    "    return(acc2x)\n",
    "\n",
    "'''\n",
    "custom_coverage\n",
    "'''\n",
    "def custom_coverage(input_bam_handle, gbk_parser_out):\n",
    "    coverage_dict = dict()\n",
    "    seq_names = list(gbk_parser_out.keys())\n",
    "    for i in seq_names:\n",
    "        input_bam_cov = input_bam_handle.count_coverage(gbk_parser_out[i][\"acc\"], \\\n",
    "                                                        gbk_parser_out[i][\"start\"], \\\n",
    "                                                        gbk_parser_out[i][\"end\"],\n",
    "                                                        quality_threshold = 0)\n",
    "        n = 0\n",
    "        for j in range (4):\n",
    "            n += len([x for x in input_bam_cov[j] if x != 0])\n",
    "            \n",
    "        coverage_dict[i] = n/(gbk_parser_out[i][\"end\"] - gbk_parser_out[i][\"start\"]) \n",
    "    return(coverage_dict)\n",
    "\n",
    "'''\n",
    "Here we define the function get_coverage\n",
    "'''\n",
    "def get_coverage(input_gbk : str = None, input_bam : str = None, \\\n",
    "                 sample_name : str = None) -> pd.DataFrame:\n",
    "\n",
    "    '''\n",
    "    input_gbk:\n",
    "        type: str \n",
    "        This a GenBank file with all the BGC sequences of a single samples \n",
    "        (contatenated in a single file) previously identified with antiSMASH. \n",
    "    input_bam:\n",
    "        Thiese are the bam fies representing the alingments of paired end reads mapped \n",
    "        on the assembled data.\n",
    "    sample_name:\n",
    "        Sample name to be used in the output (tsv) table.   \n",
    "    returns:\n",
    "        A data frame with the following columns  \n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    1 Perse GBK\n",
    "    '''\n",
    "\n",
    "    gbk_parser_out = gbk_parser(input_gbk)\n",
    "  \n",
    "    '''\n",
    "    2. Compute coverage\n",
    "    '''\n",
    "\n",
    "    input_bam_handle = pysam.AlignmentFile(input_bam, \"rb\")\n",
    "    coverage_out = custom_coverage(input_bam_handle, gbk_parser_out)\n",
    "    input_bam_handle.close()\n",
    "\n",
    "    '''\n",
    "    3. Format output\n",
    "    '''\n",
    "\n",
    "    output = dict()\n",
    "    for i in gbk_parser_out.keys():\n",
    "        gbk_parser_out[i][\"coverage\"] = coverage_out[i]\n",
    "        gbk_parser_out[i][\"sample\"] = sample_name\n",
    "        output[i] = list(gbk_parser_out[i].values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d29eb-f420-44df-94da-c1313a64d710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acbbe633-80b9-4c24-b7c4-fbedfea527dc",
   "metadata": {},
   "source": [
    "### Description of create_dataset_table function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ceebe-5f49-4495-bcc2-e1d2e228e517",
   "metadata": {},
   "source": [
    "This function reads the names of the subfolders in the input directory (i.e., metagenomic datasets), to create the datasets.tsv tables needed to run [BiG-SLICE](https://github.com/pereiramemo/bigslice) as described [here](https://github.com/medema-group/bigslice/wiki/Input-folder#datasetstsv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "881b161e-a9b1-496c-b1ba-1f34070f8e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def find_files(folder_path : str = None) -> list:\n",
    "  \n",
    "    '''\n",
    "    folder_path:\n",
    "        type: str \n",
    "        Contains the relative path from the notebook to the input folder \n",
    "    returns:\n",
    "        A list with all the files having named with the format *reion\\d+.gbk    \n",
    "        '''\n",
    "  \n",
    "    pattern = r\".*\\.region\\d+.gbk\"\n",
    "    matching_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if re.search(pattern, file):\n",
    "                matching_files.append(os.path.join(root, file))\n",
    "    return matching_files\n",
    "  \n",
    "def create_taxonomy_tables(path_to_input_folder : str = None):\n",
    "  \n",
    "    '''\n",
    "    path_to_data:\n",
    "        type: str \n",
    "        Contains the relative path from the notebook to the input folder \n",
    "        conatining the metagenomic datasets and assemblies subfolders, the \n",
    "        latter with annotated BGC sequences to be clustered.\n",
    "    returns:\n",
    "        A tsv file per sample located in a taxonomy directory created in the \n",
    "        root directory of the input folder, having the columns: \n",
    "        1. Sequence identification.\n",
    "        2. Taxonomy.\n",
    "    '''\n",
    "    \n",
    "    if path_to_input_folder == None:\n",
    "        print('You need to insert a path to the input folder')\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(path_to_input_folder):\n",
    "        print('You need to insert a valid path to the input folder')\n",
    "        return False\n",
    "    \n",
    "    '''\n",
    "    Create output directory.\n",
    "    '''\n",
    "    \n",
    "    taxomoy_dir = f\"{path_to_input_folder}/taxonomy\"\n",
    "    if not os.path.exists(taxomoy_dir):\n",
    "       os.makedirs(taxomoy_dir)\n",
    "    \n",
    "    '''\n",
    "    Crete a dictionary of lists of lists containing the data do crate the taxonomy files. \n",
    "    Each element in the dictionary will be used to create a tsv file.\n",
    "    '''\n",
    "    \n",
    "    output_dict = dict()  \n",
    "    subfolders = os.listdir(path_to_input_folder)\n",
    "    for subfolder in subfolders:\n",
    "        if subfolder != \"datasets.tsv\" and subfolder != \"taxonomy\": \n",
    "            output_dict[subfolder] = list()\n",
    "            path_to_input_subfolder = \"/\".join([path_to_input_folder, subfolder])\n",
    "            path_to_gbk_files_list = find_files(path_to_input_subfolder)\n",
    "            gbk_files = [x.split(\"/\")[-1] for x in path_to_gbk_files_list]\n",
    "            for gbk_file in gbk_files:\n",
    "                gbk_file = gbk_file.replace(\".gbk\", '')\n",
    "                output_dict[subfolder].append([gbk_file, \"NA\"])\n",
    "    \n",
    "    for key in output_dict:\n",
    "        '''\n",
    "        Define file name.\n",
    "        '''\n",
    "        output_list = output_dict[key]\n",
    "        output_tsv = f'{taxomoy_dir}/{key}_taxonomy.tsv'\n",
    "        '''\n",
    "        Write outout tsv file\n",
    "        ''' \n",
    "        try:\n",
    "            with open(output_tsv, 'w', newline='') as tsv_file:\n",
    "                tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "                for row in output_list:\n",
    "                     tsv_writer.writerow(row)  \n",
    "        except OSError as e:\n",
    "            print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea92da-4654-477d-8c29-5aa02e35d0a8",
   "metadata": {},
   "source": [
    "### Description of create_dataset_table function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f4fb6-f274-4aea-9c1a-783aab82f2b4",
   "metadata": {},
   "source": [
    "This function reads the names of the subfolders in the input directory (i.e., metagenomic datasets), and the GenBank files named with the format \"\\*.region\\d+.gbk\" as generated by [antiSMASH](https://github.com/antismash/antismash) in order the create the taxonomy files required to run [BiG-SLICE](https://github.com/pereiramemo/bigslice) as described [here](https://github.com/medema-group/bigslice/wiki/Input-folder#datasetstsv). At the moment, the function only generates the files without any taxonomic information, since the taxonomy is analyzed independently of BiG-SLICE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
