{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321d56b0-ecb4-47e9-b471-8fcee9da8a87",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a33bf-bdf4-412a-9b7f-58a4dea69722",
   "metadata": {},
   "source": [
    "## Initialization Code (Run then remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861ea586-f45c-4e37-a828-3eb8be7f5df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "if not os.path.exists('./src'):\n",
    "    os.makedirs('./src')\n",
    "    os.makedirs('./data')\n",
    "    os.makedirs('./data/input_data')\n",
    "    os.makedirs('./data/output_data')\n",
    "    os.makedirs('./data/reference_data')\n",
    "    with open('./src/requirements.txt', 'w') as f:\n",
    "        pass\n",
    "    with open('./src/utilities.ipynb', 'a') as nb:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e14e4-f345-41da-8ab4-29d41771ae56",
   "metadata": {},
   "source": [
    "## Tool Purpose  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41f9a9-6c2d-47a9-a063-5e9c5b07e0a5",
   "metadata": {},
   "source": [
    "[BiG-SLICE](https://github.com/pereiramemo/bigslice) is a tool designed to cluster BGC sequences into Gene Cluster Families (GCFs) based on their protein domain composition utilizing the [Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)](https://en.wikipedia.org/wiki/BIRCH) algorithm (which is a near-linear time complexity clustering algorithm).\n",
    "The tool can be executed in clustering mode or query mode, which perform the de novo clustering of BGC sequences and the positioning of query BGC sequences onto previously computed GCF models, respectively.   \n",
    "This notebook is dedicated to the excution of the BiG-SLICE tool utlizing the clustering mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c9dea-7126-4093-8fb3-ef15b5a6494a",
   "metadata": {},
   "source": [
    "## Input Data \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028d77b-b270-4032-949f-2d2bd47125e6",
   "metadata": {},
   "source": [
    "Input data consits of BGC sequences (complete or partial) annotated in contigs of Metagenome Assembled Genomes (MAGs), sotred as GenBank files and named fwollowing the [antiSMASH](https://github.com/antismash/antismash) or [MIBiG](https://mibig.secondarymetabolites.org/) nomenclature (i.e., <genome_name>.regionXXX.gbk and BGCXXXXXXX.gbk, respectively). \n",
    "These sequences have to be organized in a directory structure having the dataset and genomes subfolders as specified [here](https://github.com/medema-group/bigslice/wiki/Input-folder).\n",
    "This is the input data that the user must provide to run this Notebook. However, in order to being able execute the BiG-SLICE tool, here we are automatically generating the dataset.tsv and taxonomy.tsv files as described [here](https://github.com/medema-group/bigslice/wiki/Input-folder#datasetstsv).  \n",
    "For demonstration purposes, here we will be analyzing 38 metagenomics samples of the [SOLA dataset](https://pubmed.ncbi.nlm.nih.gov/29925880/), which is a time series dataset spanning three years (from 2012 to 2015) obtained from a coastal northwestern Mediterranean site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2803ae5-51dc-4a2d-b442-170a8f18412d",
   "metadata": {},
   "source": [
    "## Output Data \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85d43d-7a3c-4b46-8059-6293d0ec700a",
   "metadata": {},
   "source": [
    "Desctiption of what data is output by the tool.\n",
    "- Qualitative (Functional Capaticy, Proteomic Assembly etc)\n",
    "- File types and formatting (.fasta, Blast6, csv/dataframe with schema etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90a79e-33d2-4b4e-bbfe-9987c18b25f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb8ad4-f3f8-4bd1-a284-f4dc25b8f6a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ff5f6-8795-431d-84d8-6c9a1e46036b",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Docker](https://www.docker.com/)  \n",
    "[tidyverse R package](https://www.tidyverse.org/)  \n",
    "[RSQLite R package](https://cran.r-project.org/web/packages/RSQLite/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59918e52-950c-48a9-a885-ea9ff54cf05f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21efd60-c031-4586-bb0a-23b552a60482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install import-ipynb -q\n",
    "!Rscript -e 'install.packages(\"tidyverse\")' &> /dev/null\n",
    "!Rscript -e 'install.packages(\"RSQLite\")' &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0f871-5133-4b5c-9172-e34549d1f253",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Statements (code)(import ipynb)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09db72dd-1a0f-4ab2-8edf-8af834933099",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/epereira/workspace/dev/new_atlantis/repos/bgc_clust/src/utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from src.utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e51905b-74dd-42d9-9b66-5e1e8f56cb60",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Parameters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edee4d2-dabe-49f4-8022-761ddeeddb0a",
   "metadata": {},
   "source": [
    "Define folder for input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d9837f22-75df-4691-a9ab-5b8c1d4cd6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INPUT_DIR=./data/input_data/sola_antismash/\n",
      "env: OUTPUT_DIR=./data/output_data/\n"
     ]
    }
   ],
   "source": [
    "%env INPUT_DIR=./data/input_data/sola_antismash/\n",
    "%env OUTPUT_DIR=./data/output_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0479a5-6f3b-42e5-a7b7-87327f743d0f",
   "metadata": {},
   "source": [
    "`--cpu`: Define number of CPUs to run antiSMASH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9399b0-477f-4128-bd7d-a772586b481e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CPU=40\n"
     ]
    }
   ],
   "source": [
    "%env CPU=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6f220-6fb4-43f7-89d3-76c8f8499fba",
   "metadata": {
    "tags": []
   },
   "source": [
    "`--threshold_pct`: Calculate clustering threshold (T) based on a random sampling of pairwise distances between the data, taking the N-th percentile value as the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689130ba-7ea3-46bd-a8c4-9e5102cdf109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THRESHOLD_PCT=0.1\n"
     ]
    }
   ],
   "source": [
    "%env THRESHOLD_PCT=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087c162-c5f3-43d7-a9a1-c74d17b53355",
   "metadata": {},
   "source": [
    "## Input and output data directories \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a075fa-b2ed-404e-8482-143d5495e01f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: INPUT_DIR=./data/input_data/sola_antismash/\n",
      "env: OUTPUT_DIR=./data/output_data/\n"
     ]
    }
   ],
   "source": [
    "%env INPUT_DIR=./data/input_data/sola_antismash/\n",
    "%env OUTPUT_DIR=./data/output_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96260cdb-cfc4-4b17-b686-6aba52f13c64",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Data Precleaning (if required) \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9fc63-3946-455c-9a3a-230567324206",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once we have annotated the BGC sequences in our aseembled metagenomic samples utilizing the bgc_annot notebook, we have the following folders strucutre:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900e193-b0d5-4f5b-849f-dcc028ce0004",
   "metadata": {},
   "source": [
    "> * input_folder/\n",
    ">   * metagenomic_dataset_1/\n",
    ">     * assembly/\n",
    ">       * contig_1.region001.gbk\n",
    ">       * contig_2.region002.gbk\n",
    ">       * ...\n",
    ">   * metagenomic_dataset_2/\n",
    ">     * assembly/\n",
    ">       * contig_1.region001.gbk\n",
    ">       * contig_2.region002.gbk\n",
    ">       * ...  \n",
    ">   * ...        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db45d08f-543f-4c76-aba3-bb6e22ce7055",
   "metadata": {},
   "source": [
    "Let's check how our example dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00519864-5634-420c-8277-9e8468d3d9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/input_data/sola_antismash//ERR2604071:\n",
      "scaffolds\n",
      "\n",
      "./data/input_data/sola_antismash//ERR2604073:\n",
      "scaffolds\n",
      "\n",
      "./data/input_data/sola_antismash//ERR2604074:\n",
      "scaffolds\n",
      "\n",
      "./data/input_data/sola_antismash//ERR2604075:\n",
      "scaffolds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ls \"${INPUT_DIR}\"/* | head -12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8f9d5-dec2-40e9-9aa9-821a26ff9d6c",
   "metadata": {},
   "source": [
    "When running the following command we should see all the identified BGC sequences in GenBank format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfc46c6d-e045-4a0d-8372-4674b87fbb21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_156069.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_116291.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_33754.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_73676.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_146697.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_9801.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_111249.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_102369.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_8575.region001.gbk\n",
      "./data/input_data/sola_antismash/ERR2604088/scaffolds/ERR2604088__k119_63734.region001.gbk\n",
      "find: ‘standard output’: Broken pipe\n",
      "find: write error\n"
     ]
    }
   ],
   "source": [
    "!find \"${INPUT_DIR}\" -mindepth 3 -maxdepth 3 -type f -name \"*region*.gbk\" | head "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35853b2c-c25d-49ba-a843-fc7a1472f7fb",
   "metadata": {},
   "source": [
    "Now that we checked that we have the GenBank files and the directory has the proper structure we are going to generate the `datasets.tsv` and `taxonomy.tsv` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee7363d-231e-4e46-81b6-3ff7611837f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_dataset_table(\"./data/input_data/sola_antismash/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77aa33-4d84-4ffe-95db-7b912ed40cda",
   "metadata": {},
   "source": [
    "Next we have to create the taxonomy files. These files are only created to comply with files required to run BiG-SLICE, the taxonomic information is analyzed in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea001275-54f7-4f08-b340-77688ab68612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_taxonomy_tables(\"./data/input_data/sola_antismash/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f64e2f-e7ea-472d-9942-2010e2dcb193",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Execution of Tool \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358612e9-411d-4fba-ac26-719e4c9d9cba",
   "metadata": {},
   "source": [
    "This section aims to demonstrate how to execute the tool and performs a sample run on test data. This portion of the notebook may be fairly code intensive and is the most important part of the notebook. To improve readibility and clarity, most of the verbose code segments should be written as functions in python, as shell scripts stored in the src directory, or scripts in whichever language necessary for the tool you are using.\n",
    "\n",
    "If the step you hope to perform involves more than a couple lines of code, please see the function definition format in the src/NB_utility.ipynb and wrap your code in a function using that format. If you prefer to use bash scripts, wrtie your commands into a shell file, and then execute them in this portion of the notebook. Once your code is wrapped as function in the utility NB, you can inport and run it using the format below.\n",
    "Each step should be separated by a markdown heading with a brief explanation followed by the necessary code.\n",
    "Use the parameter variables definied earlier in the notebook as arguments for functions written here. If there is an input that is not already included in the parameters section, include it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb48725-d14d-4d5c-819e-28cd0583c13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid 102's current affinity list: 0-47\n",
      "pid 102's new affinity list: 47\n",
      "pid 103's current affinity list: 0-47\n",
      "pid 103's new affinity list: 46\n",
      "pid 104's current affinity list: 0-47\n",
      "pid 104's new affinity list: 45\n",
      "pid 105's current affinity list: 0-47\n",
      "pid 105's new affinity list: 44\n",
      "pid 106's current affinity list: 0-47\n",
      "pid 106's new affinity list: 43\n",
      "pid 107's current affinity list: 0-47\n",
      "pid 107's new affinity list: 42\n",
      "pid 108's current affinity list: 0-47\n",
      "pid 108's new affinity list: 41\n",
      "pid 109's current affinity list: 0-47\n",
      "pid 109's new affinity list: 40\n",
      "pid 110's current affinity list: 0-47\n",
      "pid 110's new affinity list: 39\n",
      "pid 111's current affinity list: 0-47\n",
      "pid 111's new affinity list: 38\n",
      "pid 112's current affinity list: 0-47\n",
      "pid 112's new affinity list: 37\n",
      "pid 113's current affinity list: 0-47\n",
      "pid 113's new affinity list: 36\n",
      "pid 114's current affinity list: 0-47\n",
      "pid 114's new affinity list: 35\n",
      "pid 115's current affinity list: 0-47\n",
      "pid 115's new affinity list: 34\n",
      "pid 116's current affinity list: 0-47\n",
      "pid 116's new affinity list: 33\n",
      "pid 117's current affinity list: 0-47\n",
      "pid 117's new affinity list: 32\n",
      "pid 118's current affinity list: 0-47\n",
      "pid 118's new affinity list: 31\n",
      "pid 119's current affinity list: 0-47\n",
      "pid 119's new affinity list: 30\n",
      "pid 120's current affinity list: 0-47\n",
      "pid 120's new affinity list: 29\n",
      "pid 121's current affinity list: 0-47\n",
      "pid 121's new affinity list: 28\n",
      "pid 122's current affinity list: 0-47\n",
      "pid 122's new affinity list: 27\n",
      "pid 123's current affinity list: 0-47\n",
      "pid 123's new affinity list: 26\n",
      "pid 124's current affinity list: 0-47\n",
      "pid 124's new affinity list: 25\n",
      "pid 125's current affinity list: 0-47\n",
      "pid 125's new affinity list: 24\n",
      "pid 126's current affinity list: 0-47\n",
      "pid 126's new affinity list: 23\n",
      "pid 127's current affinity list: 0-47\n",
      "pid 127's new affinity list: 22\n",
      "pid 128's current affinity list: 0-47\n",
      "pid 128's new affinity list: 21\n",
      "pid 129's current affinity list: 0-47\n",
      "pid 129's new affinity list: 20\n",
      "pid 130's current affinity list: 0-47\n",
      "pid 130's new affinity list: 19\n",
      "pid 131's current affinity list: 0-47\n",
      "pid 131's new affinity list: 18\n",
      "pid 132's current affinity list: 0-47\n",
      "pid 132's new affinity list: 17\n",
      "pid 133's current affinity list: 0-47\n",
      "pid 133's new affinity list: 16\n",
      "pid 134's current affinity list: 0-47\n",
      "pid 134's new affinity list: 15\n",
      "pid 135's current affinity list: 0-47\n",
      "pid 135's new affinity list: 14\n",
      "pid 136's current affinity list: 0-47\n",
      "pid 136's new affinity list: 13\n",
      "pid 137's current affinity list: 0-47\n",
      "pid 137's new affinity list: 12\n",
      "pid 138's current affinity list: 0-47\n",
      "pid 138's new affinity list: 11\n",
      "pid 139's current affinity list: 0-47\n",
      "pid 139's new affinity list: 10\n",
      "pid 140's current affinity list: 0-47\n",
      "pid 140's new affinity list: 9\n",
      "pid 141's current affinity list: 0-47\n",
      "pid 141's new affinity list: 8\n",
      "pid 1's current affinity list: 0-47\n",
      "pid 1's new affinity list: 8-47\n",
      "creating output folder...\n",
      "Loading database into memory (this can take a while)...\n",
      "[0.010746240615844727s] loading sqlite3 database\n",
      "[!!] Using customized HMM databases (if this is not intended, please reinstall your HMM databases)\n",
      "Loading HMM databases...\n",
      "[3.5056259632110596s] loading hmm databases\n",
      "processing dataset: ERR2604088...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 6033.93it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "10it [00:00, 4621.82it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.35157322883605957s] processing dataset: ERR2604088\n",
      "processing dataset: ERR2604095...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3976.02it/s]\n",
      "Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "27it [00:00, 5402.19it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.42240047454833984s] processing dataset: ERR2604095\n",
      "processing dataset: ERR2604099...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 6527.26it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "24it [00:00, 5354.43it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.4087991714477539s] processing dataset: ERR2604099\n",
      "processing dataset: ERR2604079...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 6519.21it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "28it [00:00, 5570.92it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.3569333553314209s] processing dataset: ERR2604079\n",
      "processing dataset: ERR2604073...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 4232.71it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "13it [00:00, 5265.66it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.4151763916015625s] processing dataset: ERR2604073\n",
      "processing dataset: ERR2604096...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 6427.08it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "12it [00:00, 4933.51it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.40901780128479004s] processing dataset: ERR2604096\n",
      "processing dataset: ERR2604083...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 6058.64it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "14it [00:00, 5091.50it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.3708362579345703s] processing dataset: ERR2604083\n",
      "processing dataset: ERR2604075...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3892.91it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "28it [00:00, 5627.51it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.4372525215148926s] processing dataset: ERR2604075\n",
      "processing dataset: ERR2604082...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5880.50it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "11it [00:00, 4744.20it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.377302885055542s] processing dataset: ERR2604082\n",
      "processing dataset: ERR2604106...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 4279.36it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "25it [00:00, 5500.01it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.41947436332702637s] processing dataset: ERR2604106\n",
      "processing dataset: ERR2604085...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 2295.56it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "10it [00:00, 4837.72it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.5729634761810303s] processing dataset: ERR2604085\n",
      "processing dataset: ERR2604097...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5454.39it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "9it [00:00, 4660.91it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.3844420909881592s] processing dataset: ERR2604097\n",
      "processing dataset: ERR2604108...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3433.52it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "12it [00:00, 5012.11it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.46611809730529785s] processing dataset: ERR2604108\n",
      "processing dataset: ERR2604100...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3505.39it/s]\n",
      "Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "45it [00:00, 5920.63it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.46721553802490234s] processing dataset: ERR2604100\n",
      "processing dataset: ERR2604071...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5905.52it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "7it [00:00, 4122.46it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.37815403938293457s] processing dataset: ERR2604071\n",
      "processing dataset: ERR2604109...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 4046.68it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "11it [00:00, 4817.01it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.43242454528808594s] processing dataset: ERR2604109\n",
      "processing dataset: ERR2604110...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3975.50it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "3it [00:00, 3843.28it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.4306364059448242s] processing dataset: ERR2604110\n",
      "processing dataset: ERR2604078...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5505.30it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "15it [00:00, 5244.19it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.3901338577270508s] processing dataset: ERR2604078\n",
      "processing dataset: ERR2604074...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 4058.37it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "17it [00:00, 5211.84it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.43607592582702637s] processing dataset: ERR2604074\n",
      "processing dataset: ERR2604104...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3878.56it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "32it [00:00, 5674.45it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.44185447692871094s] processing dataset: ERR2604104\n",
      "processing dataset: ERR2604081...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 6003.93it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "32it [00:00, 5628.29it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.38108301162719727s] processing dataset: ERR2604081\n",
      "processing dataset: ERR2604077...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 4100.29it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "11it [00:00, 4842.29it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.4316258430480957s] processing dataset: ERR2604077\n",
      "processing dataset: ERR2604101...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3678.55it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "25it [00:00, 5438.11it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.45654892921447754s] processing dataset: ERR2604101\n",
      "processing dataset: ERR2604107...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5238.32it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "13it [00:00, 5052.91it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.39343953132629395s] processing dataset: ERR2604107\n",
      "processing dataset: ERR2604093...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3854.77it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "35it [00:00, 5558.95it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.4514169692993164s] processing dataset: ERR2604093\n",
      "processing dataset: ERR2604084...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3659.94it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "7it [00:00, 4353.52it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.45375776290893555s] processing dataset: ERR2604084\n",
      "processing dataset: ERR2604080...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5842.58it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "33it [00:00, 5669.14it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.38031983375549316s] processing dataset: ERR2604080\n",
      "processing dataset: ERR2604102...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 4187.78it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "10it [00:00, 4911.36it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.4269139766693115s] processing dataset: ERR2604102\n",
      "processing dataset: ERR2604076...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3888.70it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "16it [00:00, 5282.91it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.4421861171722412s] processing dataset: ERR2604076\n",
      "processing dataset: ERR2604098...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5538.66it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "17it [00:00, 5326.30it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.3871631622314453s] processing dataset: ERR2604098\n",
      "processing dataset: ERR2604103...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3934.65it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "19it [00:00, 5547.63it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.4427344799041748s] processing dataset: ERR2604103\n",
      "processing dataset: ERR2604090...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3923.18it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "35it [00:00, 5498.77it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.4502878189086914s] processing dataset: ERR2604090\n",
      "processing dataset: ERR2604105...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5966.29it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "4it [00:00, 3882.72it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.37709689140319824s] processing dataset: ERR2604105\n",
      "processing dataset: ERR2604094...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3961.89it/s]\n",
      "Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "42it [00:00, 5475.08it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.4527454376220703s] processing dataset: ERR2604094\n",
      "processing dataset: ERR2604092...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3499.65it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "20it [00:00, 5594.27it/s]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.48796939849853516s] processing dataset: ERR2604092\n",
      "processing dataset: ERR2604087...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 5801.11it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "8it [00:00, 4634.59it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.3806025981903076s] processing dataset: ERR2604087\n",
      "processing dataset: ERR2604086...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3983.00it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "17it [00:00, 4955.05it/s]\n",
      "Added taxonomy info for 0 BGCs...\n",
      "[0.45091938972473145s] processing dataset: ERR2604086\n",
      "processing dataset: ERR2604091...\n",
      "Found 0 BGCs from 0 GBKs, another 745 to be parsed.\n",
      "Parsing and inserting 745 GBKs...\n",
      "100%|██████████| 745/745 [00:00<00:00, 3610.46it/s]\n",
      "0it [00:00, ?it/s]Inserted 745 new BGCs.\n",
      "Parsing and inserting taxonomy information...\n",
      "48it [00:00, 5856.09it/s]\n",
      "  0%|          | 0/28310 [00:00<?, ?it/s]Added taxonomy info for 0 BGCs...\n",
      "[0.4769167900085449s] processing dataset: ERR2604091\n",
      "Found 28310 BGC(s) from 38 dataset(s)\n",
      "Checking run status of 28310 BGCs...\n",
      "100%|██████████| 28310/28310 [00:00<00:00, 1921651.15it/s]\n",
      "[0.015189170837402344s] checking run status\n",
      "Doing biosyn_pfam scan on 28310 BGCs...\n",
      "100%|██████████| 28310/28310 [00:00<00:00, 350049.81it/s]\n",
      "  0%|          | 0/28310 [00:00<?, ?it/s]0 BGCs are already scanned in previous run\n",
      "Preparing fasta files for hmmscans...\n",
      "100%|██████████| 28310/28310 [00:01<00:00, 22525.39it/s]\n",
      "Running hmmsearch in parallel...28310 BGCs (in 284 chunks)\n",
      "100%|██████████| 28310/28310 [07:23<00:00, 63.87it/s]\n",
      "  0%|          | 0/28310 [00:00<?, ?it/s][444.586727142334s] biosyn_pfam scan\n",
      "run_status is now BIOSYN_SCANNED\n",
      "Doing sub_pfam scan on 28310 BGCs...\n",
      "100%|██████████| 28310/28310 [00:00<00:00, 65159.57it/s]\n",
      "  0%|          | 0/28310 [00:00<?, ?it/s]0 BGCs are already scanned in previous run\n",
      "Preparing fasta files for subpfam_scans...\n",
      "100%|██████████| 28310/28310 [00:41<00:00, 682.68it/s]\n",
      "Running subpfam_scans in parallel... 28310 BGCs\n",
      "100%|██████████| 27007/27007 [18:19<00:00, 24.56it/s]\n",
      "[1155.1228184700012s] sub_pfam scan\n",
      "run_status is now SUBPFAM_SCANNED\n",
      "Extracting features from 28310 BGCs...\n",
      "0 BGCs are already extracted in previous run\n",
      "Extracting features...\n",
      "100%|██████████| 28310/28310 [11:33<00:00, 40.84it/s]\n",
      "[706.794291973114s] features extraction\n",
      "run_status is now FEATURES_EXTRACTED\n",
      "Building GCF models...\n",
      "[9.98072075843811s] clustering\n",
      "run_status is now CLUSTERING_FINISHED\n",
      "Assigning GCF membership...\n",
      "[15.6200270652771s] membership_assignment\n",
      "run_status is now MEMBERSHIPS_ASSIGNED\n",
      "[1.1920928955078125e-06s] preparing_output\n",
      "run_status is now RUN_FINISHED\n",
      "Dumping in-memory database content into /output/bigslice_clust/result/data.db... 29.6660s\n",
      "BiG-SLiCE run complete!\n"
     ]
    }
   ],
   "source": [
    "!\"src/run_bigslice.sh\" cluster \\\n",
    "\"data/input_data/sola_antismash/\" \\\n",
    "\"data/output_data/bigslice_clust/\" \\\n",
    "--num_threads 40 \\\n",
    "--threshold_pct 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa663f-fc33-4af2-b25b-195edeeb5f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Data Post Processing (if required) \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d262b3-64e7-47ec-bb18-41d1691612aa",
   "metadata": {},
   "source": [
    "## Write to output directory\n",
    "---\n",
    "If the tool does not do it automatically, use this cell to write the output data to the output directory defined in the parameter section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95f5b0f-10dc-429d-82a0-e5ce3b55a7a5",
   "metadata": {},
   "source": [
    "This section aims to contain all the code necessary to perform the data cleaning, formatting or analysis that would be performed on the output of this tool. Use the same formatting as previously mentioned in the execution section of the notebook:\n",
    "- Offload long code sections to the src/Utility_NB and import the code \n",
    "- Add validation to catch errors in and irregularities in the data \n",
    "- Alternate code and markdown cells \n",
    "- Include a markdown header for each step using ### to add it to the table of contents\n",
    "- Display data and transformations where necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111075ec-19f5-4e53-acca-7eacae2ab466",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualization \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd46e51-4f2e-4811-ae5c-9ceb3f8d7046",
   "metadata": {},
   "source": [
    "If there is a visualization you would like to include here, generate it here.\n",
    "Phrase the code used to generate the visualization as a function in the format mentioned in the execution section of this notebook.\n",
    "Place the function is the utility NB such that it can be reused to generate new visualizations on future data. \n",
    "If the vizualization has additional options and parameters, there is no need to add them to the parameters section, and those parameters can be included into a miniature parameter section  in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a420e-919a-47ba-a8b5-15e2a0733e54",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "---\n",
    "Include any final parting thoughts in this section.\n",
    "This section may also incude:\n",
    "- Common mistakes and fixes. \n",
    "- Debugging tips.\n",
    "- Contact for the author.\n",
    "- Any other information you would like to include"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56339f87-34a1-4fad-a584-9ebfa2be866b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9825cda-38b4-4a11-9a35-315f96dfa27f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fc821a5-1d24-4899-ae87-24aa86d05c60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c532b35d-5cc4-48b9-a09f-2f09ebe08d27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8001bd4-2ad7-4c32-8117-cd5d5a050bd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "505e23f4-f056-4803-b336-973de2b5a061",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
